{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 224D Assignment #2\n",
    "# Part [2]: Recurrent Neural Networks\n",
    "\n",
    "This notebook will provide starter code, testing snippets, and additional guidance for implementing the Recurrent Neural Network Language Model (RNNLM) described in Part 2 of the handout.\n",
    "\n",
    "Please complete parts (a), (b), and (c) of Part 2 before beginning this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from numpy import *\n",
    "from matplotlib.pyplot import *\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['savefig.dpi'] = 100\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e): Implement a Recurrent Neural Network Language Model\n",
    "\n",
    "Follow the instructions on the handout to implement your model in `rnnlm.py`, then use the code below to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rnnlm.py:55: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if U0 == None :\n",
      "NOTE: temporarily setting self.bptt = len(y) = 3 to compute true gradient.\n",
      "grad_check: dJ/dH error norm = 2.961e-09 [ok]\n",
      "    H dims: [50, 50] = 2500 elem\n",
      "grad_check: dJ/dU error norm = 4.262e-10 [ok]\n",
      "    U dims: [10, 50] = 500 elem\n",
      "grad_check: dJ/dL[1] error norm = 1.102e-09 [ok]\n",
      "    L[1] dims: [50] = 50 elem\n",
      "grad_check: dJ/dL[2] error norm = 8.874e-10 [ok]\n",
      "    L[2] dims: [50] = 50 elem\n",
      "grad_check: dJ/dL[3] error norm = 1.152e-09 [ok]\n",
      "    L[3] dims: [50] = 50 elem\n",
      "Reset self.bptt = 4\n"
     ]
    }
   ],
   "source": [
    "from rnnlm import RNNLM\n",
    "# Gradient check on toy data, for speed\n",
    "random.seed(10)\n",
    "wv_dummy = random.randn(10,50)\n",
    "model = RNNLM(L0 = wv_dummy, U0 = wv_dummy,\n",
    "              alpha=0.005, rseed=10, bptt=4)\n",
    "model.grad_check(array([1,2,3]), array([2,3,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Vocabulary and Load PTB Data\n",
    "\n",
    "We've pre-prepared a list of the vocabulary in the Penn Treebank, along with their absolute counts and unigram frequencies. The document loader code below will \"canonicalize\" words and replace any unknowns with a `\"UUUNKKK\"` token, then convert the data to lists of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 2000 words from 38444 (84.00% of all tokens)\n"
     ]
    }
   ],
   "source": [
    "from data_utils import utils as du\n",
    "import pandas as pd\n",
    "\n",
    "# Load the vocabulary\n",
    "vocab = pd.read_table(\"data/lm/vocab.ptb.txt\", header=None, sep=\"\\s+\",\n",
    "                     index_col=0, names=['count', 'freq'], )\n",
    "\n",
    "# Choose how many top words to keep\n",
    "vocabsize = 2000\n",
    "num_to_word = dict(enumerate(vocab.index[:vocabsize]))\n",
    "word_to_num = du.invert_dict(num_to_word)\n",
    "##\n",
    "# Below needed for 'adj_loss': DO NOT CHANGE\n",
    "fraction_lost = float(sum([vocab['count'][word] for word in vocab.index\n",
    "                           if (not word in word_to_num) \n",
    "                               and (not word == \"UUUNKKK\")]))\n",
    "fraction_lost /= sum([vocab['count'][word] for word in vocab.index\n",
    "                      if (not word == \"UUUNKKK\")])\n",
    "print \"Retained %d words from %d (%.02f%% of all tokens)\" % (vocabsize, len(vocab),\n",
    "                                                             100*(1-fraction_lost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the datasets, using the vocabulary in `word_to_num`. Our starter code handles this for you, and also generates lists of lists X and Y, corresponding to input words and target words*. \n",
    "\n",
    "*(Of course, the target words are just the input words, shifted by one position, but it can be cleaner and less error-prone to keep them separate.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big investment banks refused to step up to the plate to support the beleaguered floor traders by buying big blocks of stock , traders say .\n",
      "[   4  147  169  250 1879    7 1224   64    7    1    3    7  456    1    3\n",
      " 1024  255   24  378  147    3    6   67    0  255  138    2    5]\n"
     ]
    }
   ],
   "source": [
    "# Load the training set\n",
    "docs = du.load_dataset('data/lm/ptb-train.txt')\n",
    "S_train = du.docs_to_indices(docs, word_to_num)\n",
    "X_train, Y_train = du.seqs_to_lmXY(S_train)\n",
    "\n",
    "# Load the dev set (for tuning hyperparameters)\n",
    "docs = du.load_dataset('data/lm/ptb-dev.txt')\n",
    "S_dev = du.docs_to_indices(docs, word_to_num)\n",
    "X_dev, Y_dev = du.seqs_to_lmXY(S_dev)\n",
    "\n",
    "# Load the test set (final evaluation only)\n",
    "docs = du.load_dataset('data/lm/ptb-test.txt')\n",
    "S_test = du.docs_to_indices(docs, word_to_num)\n",
    "X_test, Y_test = du.seqs_to_lmXY(S_test)\n",
    "\n",
    "# Display some sample data\n",
    "print \" \".join(d[0] for d in docs[7])\n",
    "print S_test[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f): Train and evaluate your model\n",
    "\n",
    "When you're able to pass the gradient check, let's run our model on some real language!\n",
    "\n",
    "You should randomly initialize the word vectors as Gaussian noise, i.e. $L_{ij} \\sim \\mathit{N}(0,0.1)$ and $U_{ij} \\sim \\mathit{N}(0,0.1)$; the function `random.randn` may be helpful here.\n",
    "\n",
    "As in Part 1, you should tune hyperparameters to get a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdim = 100 # dimension of hidden layer = dimension of word vectors\n",
    "random.seed(10)\n",
    "# L0 = zeros((vocabsize, hdim)) # replace with random init, \n",
    "                              # or do in RNNLM.__init__()\n",
    "L0 = sqrt(0.1) * random.randn(vocabsize, hdim)   # Gaussian noise\n",
    "# test parameters; you probably want to change these\n",
    "model = RNNLM(L0, U0 = L0, alpha=0.1, rseed=10, bptt=1)\n",
    "\n",
    "from nn.base import NNBase\n",
    "# Gradient check is going to take a *long* time here\n",
    "# since it's quadratic-time in the number of parameters.\n",
    "# run at your own risk... (but do check this!)\n",
    "# model.grad_check(array([1,2,3]), array([2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56522\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "##\n",
    "# Pare down to a smaller dataset, for speed\n",
    "# (optional - recommended to not do this for your final model)\n",
    "# using a small vocabulary of 2000-5000 words.\n",
    "ntrain = len(Y_train)\n",
    "print ntrain\n",
    "num = 5000\n",
    "X = X_train[:num]\n",
    "Y = Y_train[:num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56522\n",
      "===== Batch size 5: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 8.41326\n",
      "  Seen 500 in 80.70 s\n",
      "  [500]: mean loss 4.93806\n",
      "  Seen 1000 in 166.66 s\n",
      "  [1000]: mean loss 4.764\n",
      "  Seen 1500 in 250.21 s\n",
      "  [1500]: mean loss 4.69337\n",
      "  Seen 2000 in 328.40 s\n",
      "  [2000]: mean loss 4.58571\n",
      "  Seen 2500 in 406.14 s\n",
      "  [2500]: mean loss 4.54951\n",
      "  Seen 3000 in 485.29 s\n",
      "  [3000]: mean loss 4.38956\n",
      "  Seen 3500 in 563.50 s\n",
      "  [3500]: mean loss 4.34754\n",
      "  Seen 4000 in 642.65 s\n",
      "  [4000]: mean loss 4.39601\n",
      "  Seen 4500 in 722.28 s\n",
      "  [4500]: mean loss 4.25939\n",
      "  [5000]: mean loss 4.20713\n",
      "SGD complete: 5000 examples in 830.90 seconds.\n",
      "===== cross-entropy loss on the dev set 4.438971: =====\n",
      "===== Batch size 10: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 8.41326\n",
      "  Seen 500 in 131.99 s\n",
      "  [500]: mean loss 5.17122\n",
      "  Seen 1000 in 262.09 s\n",
      "  [1000]: mean loss 5.89807\n",
      "  Seen 1500 in 398.24 s\n",
      "  [1500]: mean loss 4.54358\n",
      "  Seen 2000 in 550.13 s\n",
      "  [2000]: mean loss 4.49413\n",
      "  [2500]: mean loss 4.40919\n",
      "SGD complete: 2500 examples in 905.35 seconds.\n",
      "===== cross-entropy loss on the dev set 4.572695: =====\n",
      "===== Batch size 15: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 8.41326\n",
      "  Seen 500 in 468.71 s\n",
      "  [500]: mean loss 4.9251\n",
      "  Seen 1000 in 929.26 s\n",
      "  [1000]: mean loss 5.22548\n",
      "SGD Interrupted: saw 1295 examples in 1114.99 seconds.\n",
      "===== cross-entropy loss on the dev set 5.340743: =====\n",
      "===== Batch size 20: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 8.41326\n",
      "  Seen 500 in 302.74 s\n",
      "  [500]: mean loss 9.25622\n",
      "  Seen 1000 in 590.99 s\n",
      "  [1000]: mean loss 8.13157"
     ]
    }
   ],
   "source": [
    "# tune the hyperparameter, batchsize, alpha, bptt\n",
    "batchsize = [5,10,15,20,25,50] # too slow if increasing the batch size. so set batchsize = 5\n",
    "\n",
    "for bs in batchsize:\n",
    "    print \"===== Batch size %d: =====\" % bs\n",
    "    model = RNNLM(L0, U0 = L0, alpha=0.1, rseed=10, bptt=1)\n",
    "    schedule = NNBase.randomiter(5*num/bs, num, bs)\n",
    "    model.train_sgd(X=X, y=Y, idxiter=schedule, printevery=500, costevery=500)\n",
    "    print \"===== cross-entropy loss on the dev set %f: =====\" % model.compute_mean_loss(X_dev, Y_dev)\n",
    "\n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Alpha 0.100000: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 8.41326\n",
      "  Seen 500 in 85.04 s\n",
      "  [500]: mean loss 4.93806\n",
      "  Seen 1000 in 164.46 s\n",
      "  [1000]: mean loss 4.764\n",
      "  Seen 1500 in 244.51 s\n",
      "  [1500]: mean loss 4.69337\n",
      "  Seen 2000 in 324.98 s\n",
      "  [2000]: mean loss 4.58571\n",
      "  Seen 2500 in 409.26 s\n",
      "  [2500]: mean loss 4.54951\n",
      "  Seen 3000 in 487.99 s\n",
      "  [3000]: mean loss 4.38956\n",
      "  Seen 3500 in 569.07 s\n",
      "  [3500]: mean loss 4.34754\n",
      "  Seen 4000 in 648.35 s\n",
      "  [4000]: mean loss 4.39601\n",
      "  Seen 4500 in 729.29 s\n",
      "  [4500]: mean loss 4.25939\n",
      "  [5000]: mean loss 4.20713\n",
      "SGD complete: 5000 examples in 842.18 seconds.\n",
      "===== cross-entropy loss on the dev set 4.438971: =====\n",
      "===== Alpha 0.010000: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 8.41326\n",
      "  Seen 500 in 81.80 s\n",
      "  [500]: mean loss 5.06165\n",
      "  Seen 1000 in 163.67 s\n",
      "  [1000]: mean loss 4.97185\n",
      "  Seen 1500 in 245.72 s\n",
      "  [1500]: mean loss 4.97417\n",
      "  Seen 2000 in 325.68 s\n",
      "  [2000]: mean loss 4.82507\n",
      "  Seen 2500 in 406.69 s\n",
      "  [2500]: mean loss 4.77953\n",
      "  Seen 3000 in 487.31 s\n",
      "  [3000]: mean loss 4.69568\n",
      "  Seen 3500 in 568.71 s\n",
      "  [3500]: mean loss 4.65309\n",
      "  Seen 4000 in 648.77 s\n",
      "  [4000]: mean loss 4.66528\n",
      "  Seen 4500 in 730.44 s\n",
      "  [4500]: mean loss 4.61035\n",
      "  [5000]: mean loss 4.55262\n",
      "SGD complete: 5000 examples in 843.88 seconds.\n",
      "===== cross-entropy loss on the dev set 4.668766: =====\n",
      "===== Alpha 0.005000: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 8.41326\n",
      "  Seen 500 in 82.45 s\n",
      "  [500]: mean loss 5.11974\n",
      "  Seen 1000 in 166.01 s\n",
      "  [1000]: mean loss 5.0376\n",
      "  Seen 1500 in 250.52 s\n",
      "  [1500]: mean loss 5.02742\n",
      "  Seen 2000 in 329.19 s\n",
      "  [2000]: mean loss 4.94873\n",
      "  Seen 2500 in 407.78 s\n",
      "  [2500]: mean loss 4.90738\n",
      "  Seen 3000 in 486.47 s\n",
      "  [3000]: mean loss 4.84282\n",
      "  Seen 3500 in 566.50 s\n",
      "  [3500]: mean loss 4.80168\n",
      "  Seen 4000 in 648.73 s\n",
      "  [4000]: mean loss 4.79167\n",
      "  Seen 4500 in 733.23 s\n",
      "  [4500]: mean loss 4.7726\n",
      "  [5000]: mean loss 4.71361\n",
      "SGD complete: 5000 examples in 848.35 seconds.\n",
      "===== cross-entropy loss on the dev set 4.802117: =====\n",
      "===== Alpha 0.001000: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 8.41326\n",
      "  Seen 500 in 84.97 s\n",
      "  [500]: mean loss 5.46546\n",
      "  Seen 1000 in 167.65 s\n",
      "  [1000]: mean loss 5.27281\n",
      "  Seen 1500 in 246.64 s\n",
      "  [1500]: mean loss 5.18299\n",
      "  Seen 2000 in 324.33 s\n",
      "  [2000]: mean loss 5.13623\n",
      "  Seen 2500 in 401.64 s\n",
      "  [2500]: mean loss 5.10128\n",
      "  Seen 3000 in 480.11 s\n",
      "  [3000]: mean loss 5.07642\n",
      "  Seen 3500 in 557.64 s\n",
      "  [3500]: mean loss 5.05668\n",
      "  Seen 4000 in 636.21 s\n",
      "  [4000]: mean loss 5.04138\n",
      "  Seen 4500 in 714.76 s\n",
      "  [4500]: mean loss 5.02936\n",
      "  [5000]: mean loss 5.01033\n",
      "SGD complete: 5000 examples in 825.11 seconds.\n",
      "===== cross-entropy loss on the dev set 5.068069: =====\n",
      "===== Alpha 0.000500: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 8.41326\n",
      "  Seen 500 in 79.55 s\n",
      "  [500]: mean loss 5.70406\n",
      "  Seen 1000 in 159.06 s\n",
      "  [1000]: mean loss 5.46528\n",
      "  Seen 1500 in 370.25 s\n",
      "  [1500]: mean loss 5.34634\n",
      "  Seen 2000 in 486.65 s\n",
      "  [2000]: mean loss 5.27313\n",
      "  Seen 2500 in 609.05 s\n",
      "  [2500]: mean loss 5.22096\n",
      "  Seen 3000 in 731.23 s\n",
      "  [3000]: mean loss 5.18323\n",
      "  Seen 3500 in 849.14 s\n",
      "  [3500]: mean loss 5.15596\n",
      "  Seen 4000 in 967.02 s\n",
      "  [4000]: mean loss 5.13305\n",
      "  Seen 4500 in 1087.50 s\n",
      "  [4500]: mean loss 5.11515\n",
      "  [5000]: mean loss 5.09839\n",
      "SGD complete: 5000 examples in 1247.47 seconds.\n",
      "===== cross-entropy loss on the dev set 5.148115: =====\n",
      "===== Alpha 0.000100: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 8.41326\n",
      "  Seen 500 in 118.96 s\n",
      "  [500]: mean loss 6.39392\n",
      "SGD Interrupted: saw 934 examples in 226.85 seconds.\n",
      "===== cross-entropy loss on the dev set 6.135649: =====\n"
     ]
    }
   ],
   "source": [
    "# tune the hyperparameter, batchsize, alpha, bptt\n",
    "bs = 5\n",
    "alphas = [0.1,0.01,0.005, 0.001, 0.0005, 0.0001] \n",
    "\n",
    "for a in alphas:\n",
    "    print \"===== Alpha %f: =====\" % a\n",
    "    model = RNNLM(L0, U0 = L0, alpha=a, rseed=10, bptt=1)\n",
    "    schedule = NNBase.randomiter(5*num/bs, num, bs)\n",
    "    model.train_sgd(X=X, y=Y, idxiter=schedule, printevery=500, costevery=500)\n",
    "    print \"===== cross-entropy loss on the dev set %f: =====\" % model.compute_mean_loss(X_dev, Y_dev)\n",
    "\n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Time_steps 2.000000: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 8.41326\n",
      "  Seen 1000 in 216.20 s\n",
      "  [1000]: mean loss 4.77212\n",
      "  Seen 2000 in 494.17 s\n",
      "  [2000]: mean loss 4.61309\n",
      "  Seen 3000 in 719.69 s\n",
      "  [3000]: mean loss 4.47269\n",
      "  Seen 4000 in 938.57 s\n",
      "  [4000]: mean loss 4.5085\n",
      "  [5000]: mean loss 4.33139\n",
      "SGD complete: 5000 examples in 1209.38 seconds.\n",
      "===== cross-entropy loss on the dev set 4.507844: =====\n",
      "===== Time_steps 3.000000: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 8.41326\n",
      "  Seen 1000 in 268.90 s\n",
      "  [1000]: mean loss 4.89763\n",
      "  Seen 2000 in 544.39 s\n",
      "  [2000]: mean loss 4.6255\n",
      "  Seen 3000 in 772.01 s\n",
      "  [3000]: mean loss 4.48653\n",
      "  Seen 4000 in 996.72 s\n",
      "  [4000]: mean loss 4.53041\n",
      "  [5000]: mean loss 4.33976\n",
      "SGD complete: 5000 examples in 1264.24 seconds.\n",
      "===== cross-entropy loss on the dev set 4.532734: =====\n",
      "===== Time_steps 4.000000: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 8.41326\n",
      "  Seen 1000 in 224.66 s\n",
      "  [1000]: mean loss 4.72919\n",
      "  Seen 2000 in 458.76 s\n",
      "  [2000]: mean loss 4.52638\n",
      "  Seen 3000 in 727.95 s\n",
      "  [3000]: mean loss 4.4066\n",
      "  Seen 4000 in 968.72 s\n",
      "  [4000]: mean loss 4.48539\n",
      "  [5000]: mean loss 4.25922\n",
      "SGD complete: 5000 examples in 1232.26 seconds.\n",
      "===== cross-entropy loss on the dev set 4.467805: =====\n",
      "===== Time_steps 5.000000: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 8.41326\n",
      "  Seen 1000 in 230.58 s\n",
      "  [1000]: mean loss 4.73273\n",
      "  Seen 2000 in 456.63 s\n",
      "  [2000]: mean loss 4.51885\n",
      "  Seen 3000 in 685.47 s\n",
      "  [3000]: mean loss 4.39694\n",
      "  Seen 4000 in 918.87 s\n",
      "  [4000]: mean loss 4.44289\n",
      "  [5000]: mean loss 4.24601\n",
      "SGD complete: 5000 examples in 1168.74 seconds.\n",
      "===== cross-entropy loss on the dev set 4.456859: =====\n"
     ]
    }
   ],
   "source": [
    "bs = 5\n",
    "alpha = 0.1\n",
    "time_steps = [2,3,4,5]\n",
    "\n",
    "for t in time_steps:\n",
    "    print \"===== Time_steps %f: =====\" % t\n",
    "    model = RNNLM(L0, U0 = L0, alpha=alpha, rseed=10, bptt=t)\n",
    "    schedule = NNBase.randomiter(5*num/bs, num, bs)\n",
    "    model.train_sgd(X=X, y=Y, idxiter=schedule, printevery=1000, costevery=1000)\n",
    "    print \"===== cross-entropy loss on the dev set %f: =====\" % model.compute_mean_loss(X_dev, Y_dev)\n",
    "\n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Evaluate cross-entropy loss on the dev set,\n",
    "## then convert to perplexity for your writeup\n",
    "model = RNNLM(L0, U0 = L0, alpha=0.1, rseed=10, bptt=1)\n",
    "schedule = NNBase.randomiter(len(Y_train), len(Y_train), 5)\n",
    "model.train_sgd(X=X_train, y=Y_train, idxiter=schedule, printevery=5000, costevery=5000)\n",
    "print \"===== cross-entropy loss on the dev set %f: =====\" % model.compute_mean_loss(X_dev, Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the model is skewed somewhat by the large number of `UUUNKKK` tokens; if these are 1/6 of the dataset, then that's a sizeable fraction that we're just waving our hands at. Naively, our model gets credit for these that's not really deserved; the formula below roughly removes this contribution from the average loss. Don't worry about how it's derived, but do report both scores - it helps us compare across models with different vocabulary sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## DO NOT CHANGE THIS CELL ##\n",
    "# Report your numbers, after computing dev_loss above.\n",
    "def adjust_loss(loss, funk, q, mode='basic'):\n",
    "    if mode == 'basic':\n",
    "        # remove freebies only: score if had no UUUNKKK\n",
    "        return (loss + funk*log(funk))/(1 - funk)\n",
    "    else:\n",
    "        # remove freebies, replace with best prediction on remaining\n",
    "        return loss + funk*log(funk) - funk*log(q)\n",
    "# q = best unigram frequency from omitted vocab\n",
    "# this is the best expected loss out of that set\n",
    "q = vocab.freq[vocabsize] / sum(vocab.freq[vocabsize:])\n",
    "print \"Unadjusted: %.03f\" % exp(dev_loss)\n",
    "print \"Adjusted for missing vocab: %.03f\" % exp(adjust_loss(dev_loss, fraction_lost, q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Save to .npy files; should only be a few MB total\n",
    "assert(min(model.sparams.L.shape) <= 100) # don't be too big\n",
    "assert(max(model.sparams.L.shape) <= 5000) # don't be too big\n",
    "save(\"rnnlm.L.npy\", model.sparams.L)\n",
    "save(\"rnnlm.U.npy\", model.params.U)\n",
    "save(\"rnnlm.H.npy\", model.params.H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (g): Generating Data\n",
    "\n",
    "Once you've trained your model to satisfaction, let's use it to generate some sentences!\n",
    "\n",
    "Implement the `generate_sequence` function in `rnnlm.py`, and call it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seq_to_words(seq):\n",
    "    return [num_to_word[s] for s in seq]\n",
    "    \n",
    "seq, J = model.generate_sequence(word_to_num[\"<s>\"], \n",
    "                                 word_to_num[\"</s>\"], \n",
    "                                 maxlen=100)\n",
    "print J\n",
    "# print seq\n",
    "print \" \".join(seq_to_words(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BONUS:** Use the unigram distribution given in the `vocab` table to fill in any `UUUNKKK` tokens in your generated sequences with words that we omitted from the vocabulary. You'll want to use `list(vocab.index)` to get a list of words, and `vocab.freq` to get a list of corresponding frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace UUUNKKK with a random unigram,\n",
    "# drawn from vocab that we skipped\n",
    "from nn.math import MultinomialSampler, multinomial_sample\n",
    "def fill_unknowns(words):\n",
    "    #### YOUR CODE HERE ####\n",
    "    ret = words # do nothing; replace this\n",
    "    \n",
    "\n",
    "    #### END YOUR CODE ####\n",
    "    return ret\n",
    "    \n",
    "print \" \".join(fill_unknowns(seq_to_words(seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
