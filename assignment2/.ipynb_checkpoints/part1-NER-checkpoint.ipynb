{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 224D Assignment #2\n",
    "# Part [1]: Deep Networks: NER Window Model\n",
    "\n",
    "For this first part of the assignment, you'll build your first \"deep\" networks. On problem set 1, you computed the backpropagation gradient $\\frac{\\partial J}{\\partial w}$ for a two-layer network; in this problem set you'll implement a slightly more complex network to perform  named entity recognition (NER).\n",
    "\n",
    "Before beginning the programming section, you should complete parts (a) and (b) of the corresponding section of the handout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from numpy import *\n",
    "from matplotlib.pyplot import *\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['savefig.dpi'] = 100\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c): Random Initialization Test\n",
    "Use the cell below to test your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.46994114 -0.83008197  0.23148553  0.43094097 -0.00258593]\n",
      " [-0.47666619 -0.52297046  0.45125243 -0.57311684 -0.71301636]\n",
      " [ 0.32105262  0.78530031 -0.85918681  0.02111762  0.54147539]]\n"
     ]
    }
   ],
   "source": [
    "from misc import random_weight_matrix\n",
    "random.seed(10)\n",
    "print random_weight_matrix(3,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d): Implementation\n",
    "\n",
    "We've provided starter code to load in the dataset and convert it to a list of \"windows\", consisting of indices into the matrix of word vectors. \n",
    "\n",
    "We pad each sentence with begin and end tokens `<s>` and `</s>`, which have their own word vector representations; additionally, we convert all words to lowercase, canonicalize digits (e.g. `1.12` becomes `DG.DGDG`), and replace unknown words with a special token `UUUNKKK`.\n",
    "\n",
    "You don't need to worry about the details of this, but you can inspect the `docs` variables or look at the raw data (in plaintext) in the `./data/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data_utils.utils as du\n",
    "import data_utils.ner as ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the starter word vectors\n",
    "wv, word_to_num, num_to_word = ner.load_wv('data/ner/vocab.txt',\n",
    "                                           'data/ner/wordVectors.txt')\n",
    "tagnames = [\"O\", \"LOC\", \"MISC\", \"ORG\", \"PER\"]\n",
    "num_to_tag = dict(enumerate(tagnames))\n",
    "tag_to_num = du.invert_dict(num_to_tag)\n",
    "\n",
    "# Set window size\n",
    "windowsize = 3\n",
    "\n",
    "# Load the training set\n",
    "docs = du.load_dataset('data/ner/train')\n",
    "X_train, y_train = du.docs_to_windows(docs, word_to_num, tag_to_num,\n",
    "                                      wsize=windowsize)\n",
    "\n",
    "# Load the dev set (for tuning hyperparameters)\n",
    "docs = du.load_dataset('data/ner/dev')\n",
    "X_dev, y_dev = du.docs_to_windows(docs, word_to_num, tag_to_num,\n",
    "                                  wsize=windowsize)\n",
    "\n",
    "# Load the test set (dummy labels only)\n",
    "docs = du.load_dataset('data/ner/test.masked')\n",
    "X_test, y_test = du.docs_to_windows(docs, word_to_num, tag_to_num,\n",
    "                                    wsize=windowsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid re-inventing the wheel, we provide a base class that handles a lot of the drudgery of managing parameters and running gradient descent. It's based on the classifier API used by [`scikit-learn`](http://scikit-learn.org/stable/), so if you're familiar with that library it should be easy to use. \n",
    "\n",
    "We'll be using this class for the rest of this assignment, so it helps to get acquainted with a simple example that should be familiar from Assignment 1. To keep this notebook uncluttered, we've put the code in the `softmax_example.py`; take a look at it there, then run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grad_check: dJ/db error norm = 3.565e-10 [ok]\n",
      "    b dims: [5] = 5 elem\n",
      "grad_check: dJ/dW error norm = 2.164e-11 [ok]\n",
      "    W dims: [5, 100] = 500 elem\n",
      "grad_check: dJ/dL[5] error norm = 2.646e-11 [ok]\n",
      "    L[5] dims: [100] = 100 elem\n"
     ]
    }
   ],
   "source": [
    "from softmax_example import SoftmaxRegression\n",
    "sr = SoftmaxRegression(wv=zeros((10,100)), dims=(100,5))\n",
    "\n",
    "##\n",
    "# Automatic gradient checker!\n",
    "# this checks anything you add to self.grads or self.sgrads\n",
    "# using the method of Assignment 1\n",
    "sr.grad_check(x=5, y=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement a model, you need to subclass `NNBase`, then implement the following methods:\n",
    "\n",
    "- `__init__()` (initialize parameters and hyperparameters)\n",
    "- `_acc_grads()` (compute and accumulate gradients)\n",
    "- `compute_loss()` (compute loss for a training example)\n",
    "- `predict()`, `predict_proba()`, or other prediction method (for evaluation)\n",
    "\n",
    "`NNBase` provides you with a few others that will be helpful:\n",
    "\n",
    "- `grad_check()` (run a gradient check - calls `_acc_grads` and `compute_loss`)\n",
    "- `train_sgd()` (run SGD training; more on this later)\n",
    "\n",
    "Your task is to implement the window model in `nerwindow.py`; a scaffold has been provided for you with instructions on what to fill in.\n",
    "\n",
    "When ready, you can test below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grad_check: dJ/db2 error norm = 3.233e-10 [ok]\n",
      "    b2 dims: [5] = 5 elem\n",
      "grad_check: dJ/dU error norm = 2.847e-10 [ok]\n",
      "    U dims: [5, 100] = 500 elem\n",
      "grad_check: dJ/db1 error norm = 2.849e-09 [ok]\n",
      "    b1 dims: [100] = 100 elem\n",
      "grad_check: dJ/dW error norm = 1.337e-08 [ok]\n",
      "    W dims: [100, 150] = 15000 elem\n",
      "grad_check: dJ/dL[30] error norm = 3.568e-11 [ok]\n",
      "    L[30] dims: [50] = 50 elem\n",
      "grad_check: dJ/dL[6659] error norm = 4.442e-11 [ok]\n",
      "    L[6659] dims: [50] = 50 elem\n",
      "grad_check: dJ/dL[12637] error norm = 4.6e-11 [ok]\n",
      "    L[12637] dims: [50] = 50 elem\n"
     ]
    }
   ],
   "source": [
    "from nerwindow import WindowMLP\n",
    "clf = WindowMLP(wv, windowsize=3, dims=[None, 100, 5],\n",
    "                reg=0.001, alpha=0.01)\n",
    "clf.grad_check(X_train[0], y_train[0]) # gradient check on single point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train your model on some data! You can implement your own SGD method, but we recommend that you just call `clf.train_sgd`. This takes the following arguments:\n",
    "\n",
    "- `X`, `y` : training data\n",
    "- `idxiter`: iterable (list or generator) that gives index (row of X) of training examples in the order they should be visited by SGD\n",
    "- `printevery`: int, prints progress after this many examples\n",
    "- `costevery`: int, computes mean loss after this many examples. This is a costly operation, so don't make this too frequent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nepoch = 5\n",
    "N = nepoch * len(y_train)\n",
    "k = 5 # minibatch size\n",
    "\n",
    "random.seed(10) # do not change this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.77621\n",
      "  Seen 10000 in 7.01 s\n",
      "  [10000]: mean loss 0.492621\n",
      "  Seen 20000 in 13.90 s\n",
      "  [20000]: mean loss 0.432389\n",
      "  Seen 30000 in 20.39 s\n",
      "  [30000]: mean loss 0.456467\n",
      "  Seen 40000 in 27.16 s\n",
      "  [40000]: mean loss 0.480589\n",
      "  Seen 50000 in 33.63 s\n",
      "  [50000]: mean loss 0.48871\n",
      "  Seen 60000 in 40.02 s\n",
      "  [60000]: mean loss 0.403107\n",
      "  Seen 70000 in 46.49 s\n",
      "  [70000]: mean loss 0.535657\n",
      "  Seen 80000 in 52.83 s\n",
      "  [80000]: mean loss 0.374414\n",
      "  Seen 90000 in 59.18 s\n",
      "  [90000]: mean loss 0.389194\n",
      "  [100000]: mean loss 0.345707\n",
      "SGD complete: 100000 examples in 69.91 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1.7762140406392297),\n",
       " (10000, 0.49262096676416761),\n",
       " (20000, 0.43238885492935808),\n",
       " (30000, 0.45646657689840225),\n",
       " (40000, 0.48058857306466596),\n",
       " (50000, 0.48871001440198625),\n",
       " (60000, 0.40310735445654866),\n",
       " (70000, 0.53565694416484122),\n",
       " (80000, 0.37441410368123768),\n",
       " (90000, 0.38919416767932213),\n",
       " (100000, 0.3457072947798453)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#### YOUR CODE HERE ####\n",
    "clf.train_sgd(X_train[:100000], y_train[:100000])\n",
    "#### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation we give you supports minibatch learning; if `idxiter` is a list-of-lists (or yields lists), then gradients will be computed for all indices in a minibatch before modifying the parameters \n",
    "(this is why we have you write `_acc_grad` instead of applying them directly!).\n",
    "\n",
    "Before training, you should generate a training schedule to pass as `idxiter`. If you know how to use Python generators, we recommend those; otherwise, just make a static list. Make the following in the cell below:\n",
    "\n",
    "- An \"epoch\" schedule that just iterates through the training set, in order, `nepoch` times.\n",
    "- A random schedule of `N` examples sampled with replacement from the training set.\n",
    "- A random schedule of `N/k` minibatches of size `k`, sampled with replacement from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<itertools.chain object at 0x10ea88ed0>\n",
      "203620\n"
     ]
    }
   ],
   "source": [
    "# print len(y_train)\n",
    "from nn.base import NNBase\n",
    "import itertools\n",
    "# training schedule\n",
    "from nerwindow import WindowMLP\n",
    "from nn.base import NNBase\n",
    "from nerwindow import full_report, eval_performance\n",
    "\n",
    "# An \"epoch\" schedule that just iterates through the training set, in order, nepoch times.\n",
    "idxiter_epoch = NNBase.epochiter(len(y_train), 1)\n",
    "print idxiter_epoch\n",
    "\n",
    "big = 0\n",
    "\n",
    "for i in itertools.izip(idxiter_epoch):\n",
    "    idx = i[0]\n",
    "    if idx > big:\n",
    "        big = idx\n",
    "\n",
    "print big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training schedule 0: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.77988\n",
      "  [30000]: mean loss 0.437103\n",
      "  [60000]: mean loss 0.383579\n",
      "  [90000]: mean loss 0.386789\n",
      "  [120000]: mean loss 0.347486\n",
      "  [150000]: mean loss 0.386805\n",
      "  [180000]: mean loss 0.305826\n",
      "  [210000]: mean loss 0.33048\n",
      "  [240000]: mean loss 0.314484\n",
      "  [270000]: mean loss 0.505674\n",
      "  Seen 300000 in 139.57 s\n",
      "  [300000]: mean loss 0.311091\n",
      "  [330000]: mean loss 0.266101\n",
      "  [360000]: mean loss 0.30446\n",
      "  [390000]: mean loss 0.26948\n",
      "  [420000]: mean loss 0.263542\n",
      "  [450000]: mean loss 0.245656\n",
      "  [480000]: mean loss 0.264812\n",
      "  [510000]: mean loss 0.236556\n",
      "  [540000]: mean loss 0.261287\n",
      "  [570000]: mean loss 0.204602\n",
      "  Seen 600000 in 278.84 s\n",
      "  [600000]: mean loss 0.231522\n",
      "  [630000]: mean loss 0.31804\n",
      "  [660000]: mean loss 0.303524\n",
      "  [690000]: mean loss 0.298685\n",
      "  [720000]: mean loss 0.218489\n",
      "  [750000]: mean loss 0.302339\n",
      "  [780000]: mean loss 0.222141\n",
      "  [810000]: mean loss 0.217043\n",
      "  [840000]: mean loss 0.203598\n",
      "  [870000]: mean loss 0.285822\n",
      "  Seen 900000 in 418.60 s\n",
      "  [900000]: mean loss 0.194441\n",
      "  [930000]: mean loss 0.209087\n",
      "  [960000]: mean loss 0.200125\n",
      "  [990000]: mean loss 0.189848\n",
      "  [1018105]: mean loss 0.19244\n",
      "SGD complete: 1018105 examples in 482.49 seconds.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.96      0.97      0.96     42759\n",
      "        LOC       0.80      0.81      0.80      2094\n",
      "       MISC       0.85      0.62      0.72      1268\n",
      "        ORG       0.76      0.37      0.49      2092\n",
      "        PER       0.65      0.86      0.74      3149\n",
      "\n",
      "avg / total       0.92      0.92      0.92     51362\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  74.30%\n",
      "Mean recall:     69.05%\n",
      "Mean F1:         69.21%\n",
      "===== Training schedule 1: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.77988\n",
      "  [30000]: mean loss 0.338837\n",
      "  [60000]: mean loss 0.308733\n",
      "  [90000]: mean loss 0.292236\n",
      "  [120000]: mean loss 0.269446\n",
      "  [150000]: mean loss 0.257046\n",
      "  [180000]: mean loss 0.261258\n",
      "  [210000]: mean loss 0.231111\n",
      "  [240000]: mean loss 0.226236\n",
      "  [270000]: mean loss 0.210666\n",
      "  Seen 300000 in 141.30 s\n",
      "  [300000]: mean loss 0.206172\n",
      "  [330000]: mean loss 0.203375\n",
      "  [360000]: mean loss 0.195236\n",
      "  [390000]: mean loss 0.19684\n",
      "  [420000]: mean loss 0.185833\n",
      "  [450000]: mean loss 0.177136\n",
      "  [480000]: mean loss 0.1725\n",
      "  [510000]: mean loss 0.167828\n",
      "  [540000]: mean loss 0.163379\n",
      "  [570000]: mean loss 0.159622\n",
      "  Seen 600000 in 282.40 s\n",
      "  [600000]: mean loss 0.157201\n",
      "  [630000]: mean loss 0.154761\n",
      "  [660000]: mean loss 0.154338\n",
      "  [690000]: mean loss 0.146479\n",
      "  [720000]: mean loss 0.147324\n",
      "  [750000]: mean loss 0.144755\n",
      "  [780000]: mean loss 0.14083\n",
      "  [810000]: mean loss 0.136905\n",
      "  [840000]: mean loss 0.143235\n",
      "  [870000]: mean loss 0.142714\n",
      "  Seen 900000 in 442.59 s\n",
      "  [900000]: mean loss 0.140712\n",
      "  [930000]: mean loss 0.132376\n",
      "  [960000]: mean loss 0.128192\n",
      "  [990000]: mean loss 0.13046\n",
      "  [1018105]: mean loss 0.128987\n",
      "SGD complete: 1018105 examples in 524.62 seconds.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.97      0.98      0.97     42759\n",
      "        LOC       0.91      0.77      0.83      2094\n",
      "       MISC       0.82      0.69      0.75      1268\n",
      "        ORG       0.70      0.63      0.66      2092\n",
      "        PER       0.85      0.84      0.84      3149\n",
      "\n",
      "avg / total       0.94      0.94      0.94     51362\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  82.52%\n",
      "Mean recall:     74.64%\n",
      "Mean F1:         78.30%\n",
      "===== Training schedule 2: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.77988\n",
      "  [30000]: mean loss 0.257025\n",
      "  [60000]: mean loss 0.206504\n",
      "  [90000]: mean loss 0.176938\n",
      "  [120000]: mean loss 0.157037\n",
      "  [150000]: mean loss 0.144598\n",
      "  [180000]: mean loss 0.140693\n",
      "  [203621]: mean loss 0.129192\n",
      "SGD complete: 203621 examples in 415.11 seconds.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.97      0.98      0.97     42759\n",
      "        LOC       0.91      0.77      0.83      2094\n",
      "       MISC       0.82      0.69      0.75      1268\n",
      "        ORG       0.70      0.62      0.66      2092\n",
      "        PER       0.85      0.84      0.84      3149\n",
      "\n",
      "avg / total       0.94      0.94      0.94     51362\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  82.42%\n",
      "Mean recall:     74.58%\n",
      "Mean F1:         78.22%\n"
     ]
    }
   ],
   "source": [
    "# training schedule\n",
    "\n",
    "# An \"epoch\" schedule that just iterates through the training set, in order, nepoch times.\n",
    "idxiter_epoch = NNBase.epochiter(len(y_train), nepoch)\n",
    "\n",
    "# A random schedule of N examples sampled with replacement from the training set.\n",
    "idxiter_random = NNBase.randomiter(N, len(y_train), 1)\n",
    "\n",
    "# A random schedule of N/k minibatches of size k, sampled with replacement from the training set.\n",
    "idxiter_minibatch = NNBase.randomiter(N / k, len(y_train), k)\n",
    "\n",
    "schedules = [idxiter_epoch, idxiter_random, idxiter_minibatch]\n",
    "\n",
    "from nerwindow import full_report, eval_performance\n",
    "\n",
    "for i, schedule in enumerate(schedules):\n",
    "    print \"===== Training schedule %d: =====\" % i\n",
    "    clf = WindowMLP(wv, windowsize=3, dims=[None, 100, 5], reg=0.001, alpha=0.01)\n",
    "    clf.train_sgd(X=X_train, y=y_train, idxiter=schedule, printevery=300000, costevery=30000)\n",
    "    yp = clf.predict(X_dev)\n",
    "    full_report(y_dev, yp, tagnames)\n",
    "    eval_performance(y_dev, yp, tagnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call `train_sgd` to train on `X_train`, `y_train`. To verify that things work, train on 100,000 examples or so to start (with any of the above schedules). This shouldn't take more than a couple minutes, and you should get a mean cross-entropy loss around 0.4.\n",
    "\n",
    "Now, if this works well, it's time for production! You have three tasks here:\n",
    "\n",
    "1. Train a good model\n",
    "2. Plot a learning curve (cost vs. # of iterations)\n",
    "3. Use your best model to predict the test set\n",
    "\n",
    "You should train on the `train` data and evaluate performance on the `dev` set. The `test` data we provided has only dummy labels (everything is `O`); we'll compare your predictions to the true labels at grading time. \n",
    "\n",
    "Scroll down to section (f) for the evaluation code.\n",
    "\n",
    "We don't expect you to spend too much time doing an exhaustive search here; the default parameters should work well, although you can certainly do better. Try to achieve an F1 score of at least 76% on the dev set, as reported by `eval_performance`.\n",
    "\n",
    "Feel free to create new cells and write new code here, including new functions (helpers and otherwise) in `nerwindow.py`. When you have a good model, follow the instructions below to make predictions on the test set.\n",
    "\n",
    "A strong model may require 10-20 passes (or equivalent number of random samples) through the training set and could take 20 minutes or more to train - but it's also possible to be much, much faster!\n",
    "\n",
    "Things you may want to tune:\n",
    "- `alpha` (including using an \"annealing\" schedule to decrease the learning rate over time)\n",
    "- training schedule and minibatch size\n",
    "- regularization strength\n",
    "- hidden layer dimension\n",
    "- width of context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Alpha 0.010000: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.77988\n",
      "  [200]: mean loss 0.458443\n",
      "  [400]: mean loss 0.433393\n",
      "  [600]: mean loss 0.414653\n",
      "  [800]: mean loss 0.415561\n",
      "  Seen 1000 in 44.26 s\n",
      "  [1000]: mean loss 0.394092\n",
      "  [1200]: mean loss 0.387008\n",
      "  [1400]: mean loss 0.387926\n",
      "  [1600]: mean loss 0.380776\n",
      "  [1800]: mean loss 0.384529\n",
      "  Seen 2000 in 88.40 s\n",
      "  [2000]: mean loss 0.376289\n",
      "  [2200]: mean loss 0.367217\n",
      "  [2400]: mean loss 0.378007\n",
      "  [2600]: mean loss 0.364172\n",
      "  [2800]: mean loss 0.360213\n",
      "  Seen 3000 in 132.75 s\n",
      "  [3000]: mean loss 0.356571\n",
      "  [3200]: mean loss 0.359281\n",
      "  [3400]: mean loss 0.355689\n",
      "  [3600]: mean loss 0.353342\n",
      "  [3800]: mean loss 0.349917\n",
      "  Seen 4000 in 214.38 s\n",
      "  [4000]: mean loss 0.349598\n",
      "  [4200]: mean loss 0.347983\n",
      "  [4400]: mean loss 0.359116\n",
      "  [4600]: mean loss 0.346698\n",
      "  [4800]: mean loss 0.363263\n",
      "  Seen 5000 in 307.13 s\n",
      "  [5000]: mean loss 0.347247\n",
      "  [5200]: mean loss 0.343579\n",
      "  [5400]: mean loss 0.340457\n",
      "  [5600]: mean loss 0.339013\n",
      "  [5800]: mean loss 0.333272\n",
      "  Seen 6000 in 395.48 s\n",
      "  [6000]: mean loss 0.339263\n",
      "  [6200]: mean loss 0.331303\n",
      "  [6400]: mean loss 0.328342\n",
      "  [6600]: mean loss 0.32791\n",
      "  [6800]: mean loss 0.336065\n",
      "  Seen 7000 in 487.72 s\n",
      "  [7000]: mean loss 0.337181\n",
      "  [7200]: mean loss 0.329737\n",
      "  [7400]: mean loss 0.331004\n",
      "  [7600]: mean loss 0.329081\n",
      "  [7800]: mean loss 0.329078\n",
      "  Seen 8000 in 546.42 s\n",
      "  [8000]: mean loss 0.322355\n",
      "  [8200]: mean loss 0.329039\n",
      "  [8400]: mean loss 0.319848\n",
      "  [8600]: mean loss 0.336211\n",
      "  [8800]: mean loss 0.317696\n",
      "  Seen 9000 in 605.21 s\n",
      "  [9000]: mean loss 0.328798\n",
      "  [9200]: mean loss 0.322377\n",
      "  [9400]: mean loss 0.317438\n",
      "  [9600]: mean loss 0.324252\n",
      "  [9800]: mean loss 0.322635\n",
      "  [10000]: mean loss 0.317054\n",
      "SGD complete: 10000 examples in 679.30 seconds.\n",
      "===== Alpha 0.100000: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.77988\n",
      "  [200]: mean loss 0.960732\n",
      "  [400]: mean loss 0.916942\n",
      "  [600]: mean loss 0.743754\n",
      "  [800]: mean loss 0.681796\n",
      "  Seen 1000 in 61.29 s\n",
      "  [1000]: mean loss 0.677929\n",
      "  [1200]: mean loss 0.55703\n",
      "  [1400]: mean loss 0.754953\n",
      "  [1600]: mean loss 0.638967\n",
      "  [1800]: mean loss 1.05232\n",
      "  Seen 2000 in 121.93 s\n",
      "  [2000]: mean loss 0.837269\n",
      "  [2200]: mean loss 0.681314\n",
      "  [2400]: mean loss 0.810516\n",
      "  [2600]: mean loss 0.662452\n",
      "  [2800]: mean loss 0.907802\n",
      "  Seen 3000 in 182.32 s\n",
      "  [3000]: mean loss 0.594015\n",
      "  [3200]: mean loss 0.601474\n",
      "  [3400]: mean loss 0.607156\n",
      "  [3600]: mean loss 0.800824\n",
      "  [3800]: mean loss 0.679456\n",
      "  Seen 4000 in 243.10 s\n",
      "  [4000]: mean loss 0.975868\n",
      "  [4200]: mean loss 1.13093\n",
      "  [4400]: mean loss 0.621935\n",
      "  [4600]: mean loss 0.584677\n",
      "  [4800]: mean loss 0.61708\n",
      "  Seen 5000 in 304.03 s\n",
      "  [5000]: mean loss 0.696199\n",
      "  [5200]: mean loss 0.649533\n",
      "  [5400]: mean loss 0.587462\n",
      "  [5600]: mean loss 0.629325\n",
      "  [5800]: mean loss 0.627616\n",
      "  Seen 6000 in 364.96 s\n",
      "  [6000]: mean loss 0.556836\n",
      "  [6200]: mean loss 0.561677\n",
      "  [6400]: mean loss 0.657052\n",
      "  [6600]: mean loss 0.533014\n",
      "  [6800]: mean loss 0.565565\n",
      "  Seen 7000 in 426.09 s\n",
      "  [7000]: mean loss 0.508441\n",
      "  [7200]: mean loss 0.631424\n",
      "  [7400]: mean loss 0.526233\n",
      "  [7600]: mean loss 0.615813\n",
      "  [7800]: mean loss 0.52872\n",
      "  Seen 8000 in 485.84 s\n",
      "  [8000]: mean loss 0.894394\n",
      "  [8200]: mean loss 0.464872\n",
      "  [8400]: mean loss 0.567791\n",
      "  [8600]: mean loss 0.532384\n",
      "  [8800]: mean loss 0.463411\n",
      "  Seen 9000 in 546.82 s\n",
      "  [9000]: mean loss 0.495852\n",
      "  [9200]: mean loss 0.88916\n",
      "  [9400]: mean loss 0.441663\n",
      "  [9600]: mean loss 0.408677\n",
      "  [9800]: mean loss 0.470798\n",
      "  [10000]: mean loss 0.418893\n",
      "SGD complete: 10000 examples in 619.61 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Model 2 minibatch is the best\n",
    "# schedule = idxiter_minibatch\n",
    "# To simplify the tunning process, only using one epoch \n",
    "\n",
    "from nerwindow import full_report, eval_performance\n",
    "# Annealing schedule\n",
    "#### YOUR CODE HERE ####\n",
    "# Sandbox: build a good model by tuning hyperparameters\n",
    "# annealing_tuning = [0.01, 0.001, 0.0001]         # try to use AdaGrad to avoid tuning learning rate\n",
    "# by observing the first SGD training, the cost is slightly increasing every 30000 samples\n",
    "# besides F1 score already reaches 80% when learning_rate = 0.01\n",
    "# annealing_nepoch = [30000, 60000, 90000]\n",
    "\n",
    "# schedule = NNBase.randomiter(10000, len(y_train), 5)  # k = 5\n",
    "alpha1 = 0.01\n",
    "alpha2 = 0.1\n",
    "clf1 = WindowMLP(wv, windowsize=3, dims=[None, 100, 5], reg=0.001, alpha=alpha1)\n",
    "clf2 = WindowMLP(wv, windowsize=3, dims=[None, 100, 5], reg=0.001, alpha=alpha2)\n",
    "\n",
    "print \"===== Alpha %f: =====\" % alpha1\n",
    "trainingcurve1 = clf1.train_sgd(X=X_train, y=y_train, idxiter=NNBase.randomiter(10000, len(y_train), 5),\n",
    "                                printevery=1000, costevery=200)\n",
    "print \"===== Alpha %f: =====\" % alpha2\n",
    "trainingcurve2 = clf2.train_sgd(X=X_train, y=y_train, idxiter=NNBase.randomiter(10000, len(y_train), 5),\n",
    "                                printevery=1000, costevery=200)\n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Batch Size 5: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.77988\n",
      "  Seen 10000 in 17.35 s\n",
      "  [10000]: mean loss 0.317054\n",
      "  Seen 20000 in 34.67 s\n",
      "  [20000]: mean loss 0.287443\n",
      "  Seen 30000 in 51.91 s\n",
      "  [30000]: mean loss 0.257025\n",
      "  Seen 40000 in 69.13 s\n",
      "  [40000]: mean loss 0.238609\n",
      "  Seen 50000 in 86.37 s\n",
      "  [50000]: mean loss 0.223832\n",
      "  Seen 60000 in 103.60 s\n",
      "  [60000]: mean loss 0.206504\n",
      "  Seen 70000 in 120.85 s\n",
      "  [70000]: mean loss 0.196983\n",
      "  Seen 80000 in 138.03 s\n",
      "  [80000]: mean loss 0.183733\n",
      "  Seen 90000 in 155.18 s\n",
      "  [90000]: mean loss 0.176938\n",
      "  Seen 100000 in 172.34 s\n",
      "  [100000]: mean loss 0.168766\n",
      "  Seen 110000 in 189.53 s\n",
      "  [110000]: mean loss 0.160299\n",
      "  Seen 120000 in 206.75 s\n",
      "  [120000]: mean loss 0.157037\n",
      "  Seen 130000 in 223.92 s\n",
      "  [130000]: mean loss 0.15408\n",
      "  Seen 140000 in 241.13 s\n",
      "  [140000]: mean loss 0.148994\n",
      "  Seen 150000 in 258.30 s\n",
      "  [150000]: mean loss 0.144598\n",
      "  Seen 160000 in 275.47 s\n",
      "  [160000]: mean loss 0.138114\n",
      "  Seen 170000 in 292.63 s\n",
      "  [170000]: mean loss 0.135947\n",
      "  Seen 180000 in 309.85 s\n",
      "  [180000]: mean loss 0.140693\n",
      "  Seen 190000 in 327.01 s\n",
      "  [190000]: mean loss 0.135142\n",
      "  Seen 200000 in 344.13 s\n",
      "  [200000]: mean loss 0.126871\n",
      "  [203621]: mean loss 0.129192\n",
      "SGD complete: 203621 examples in 365.05 seconds.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.97      0.98      0.97     42759\n",
      "        LOC       0.91      0.77      0.83      2094\n",
      "       MISC       0.82      0.69      0.75      1268\n",
      "        ORG       0.70      0.62      0.66      2092\n",
      "        PER       0.85      0.84      0.84      3149\n",
      "\n",
      "avg / total       0.94      0.94      0.94     51362\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  82.42%\n",
      "Mean recall:     74.58%\n",
      "Mean F1:         78.22%\n",
      "===== Batch Size 10: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.77988\n",
      "  Seen 10000 in 26.45 s\n",
      "  [10000]: mean loss 0.287316\n",
      "  Seen 20000 in 51.29 s\n",
      "  [20000]: mean loss 0.238631\n",
      "  Seen 30000 in 76.00 s\n",
      "  [30000]: mean loss 0.206654\n",
      "  Seen 40000 in 100.71 s\n",
      "  [40000]: mean loss 0.183714\n",
      "  Seen 50000 in 125.48 s\n",
      "  [50000]: mean loss 0.168974\n",
      "  Seen 60000 in 150.12 s\n",
      "  [60000]: mean loss 0.157216\n",
      "  Seen 70000 in 174.80 s\n",
      "  [70000]: mean loss 0.149135\n",
      "  Seen 80000 in 205.94 s\n",
      "  [80000]: mean loss 0.137853\n",
      "  Seen 90000 in 235.53 s\n",
      "  [90000]: mean loss 0.140519\n",
      "  Seen 100000 in 264.27 s\n",
      "  [100000]: mean loss 0.126912\n",
      "  [101810]: mean loss 0.128694\n",
      "SGD complete: 101810 examples in 285.73 seconds.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.97      0.98      0.98     42759\n",
      "        LOC       0.90      0.77      0.83      2094\n",
      "       MISC       0.82      0.69      0.75      1268\n",
      "        ORG       0.71      0.62      0.66      2092\n",
      "        PER       0.85      0.84      0.84      3149\n",
      "\n",
      "avg / total       0.94      0.94      0.94     51362\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  82.48%\n",
      "Mean recall:     74.82%\n",
      "Mean F1:         78.38%\n",
      "===== Batch Size 15: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.77988\n",
      "  Seen 10000 in 44.18 s\n",
      "  [10000]: mean loss 0.257223\n",
      "  Seen 20000 in 92.98 s\n",
      "  [20000]: mean loss 0.206785\n",
      "  Seen 30000 in 143.79 s\n",
      "  [30000]: mean loss 0.177692\n",
      "  Seen 40000 in 194.86 s\n",
      "  [40000]: mean loss 0.157495\n",
      "  Seen 50000 in 241.52 s\n",
      "  [50000]: mean loss 0.144961\n",
      "  Seen 60000 in 282.82 s\n",
      "  [60000]: mean loss 0.142009\n",
      "  [67873]: mean loss 0.129396\n",
      "SGD complete: 67873 examples in 324.88 seconds.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.97      0.98      0.98     42759\n",
      "        LOC       0.90      0.78      0.83      2094\n",
      "       MISC       0.83      0.69      0.75      1268\n",
      "        ORG       0.70      0.62      0.66      2092\n",
      "        PER       0.85      0.84      0.85      3149\n",
      "\n",
      "avg / total       0.94      0.94      0.94     51362\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  82.47%\n",
      "Mean recall:     74.97%\n",
      "Mean F1:         78.45%\n",
      "===== Batch Size 20: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.77988\n",
      "  Seen 10000 in 43.97 s\n",
      "  [10000]: mean loss 0.238372\n",
      "  Seen 20000 in 85.85 s\n",
      "  [20000]: mean loss 0.184117\n",
      "  Seen 30000 in 128.13 s\n",
      "  [30000]: mean loss 0.157692\n",
      "  Seen 40000 in 172.86 s\n",
      "  [40000]: mean loss 0.137339\n",
      "  Seen 50000 in 222.98 s\n",
      "  [50000]: mean loss 0.127035\n",
      "  [50905]: mean loss 0.128631\n",
      "SGD complete: 50905 examples in 247.76 seconds.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.97      0.98      0.97     42759\n",
      "        LOC       0.90      0.77      0.83      2094\n",
      "       MISC       0.84      0.69      0.76      1268\n",
      "        ORG       0.70      0.62      0.66      2092\n",
      "        PER       0.85      0.83      0.84      3149\n",
      "\n",
      "avg / total       0.94      0.94      0.94     51362\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  82.53%\n",
      "Mean recall:     74.32%\n",
      "Mean F1:         78.12%\n",
      "===== Batch Size 25: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.77988\n",
      "  Seen 10000 in 51.08 s\n",
      "  [10000]: mean loss 0.224718\n",
      "  Seen 20000 in 101.84 s\n",
      "  [20000]: mean loss 0.169044\n",
      "  Seen 30000 in 149.27 s\n",
      "  [30000]: mean loss 0.145005\n",
      "  Seen 40000 in 196.78 s\n",
      "  [40000]: mean loss 0.127767\n",
      "  [40724]: mean loss 0.129319\n",
      "SGD complete: 40724 examples in 217.53 seconds.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.97      0.98      0.98     42759\n",
      "        LOC       0.91      0.77      0.83      2094\n",
      "       MISC       0.83      0.69      0.76      1268\n",
      "        ORG       0.71      0.62      0.66      2092\n",
      "        PER       0.85      0.85      0.85      3149\n",
      "\n",
      "avg / total       0.94      0.95      0.94     51362\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  82.54%\n",
      "Mean recall:     75.18%\n",
      "Mean F1:         78.57%\n",
      "===== Batch Size 50: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.77988\n",
      "  Seen 10000 in 85.07 s\n",
      "  [10000]: mean loss 0.168526\n",
      "  Seen 20000 in 169.98 s\n",
      "  [20000]: mean loss 0.126981\n",
      "  [20362]: mean loss 0.13019\n",
      "SGD complete: 20362 examples in 190.76 seconds.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.97      0.98      0.98     42759\n",
      "        LOC       0.90      0.77      0.83      2094\n",
      "       MISC       0.83      0.69      0.75      1268\n",
      "        ORG       0.70      0.62      0.66      2092\n",
      "        PER       0.84      0.85      0.85      3149\n",
      "\n",
      "avg / total       0.94      0.94      0.94     51362\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  82.01%\n",
      "Mean recall:     75.26%\n",
      "Mean F1:         78.36%\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# schedule = NNBase.randomiter(N/k, len(y_train), k)  # k = 5\n",
    "from nerwindow import full_report, eval_performance\n",
    "# Annealing schedule\n",
    "#### YOUR CODE HERE ####\n",
    "# Sandbox: build a good model by tuning hyperparameters\n",
    "batch_size = [5, 10, 15, 20, 25, 50]\n",
    "for s in batch_size:\n",
    "    print \"===== Batch Size %d: =====\" % s \n",
    "    schedule = NNBase.randomiter(N/s, len(y_train), s)\n",
    "    clf = WindowMLP(wv, windowsize=3, dims=[None, 100, 5], reg=0.001, alpha=0.01)\n",
    "    clf.train_sgd(X=X_train, y=y_train, idxiter=schedule, printevery=10000, costevery=10000)\n",
    "    yp = clf.predict(X_dev)\n",
    "    full_report(y_dev, yp, tagnames)\n",
    "    eval_performance(y_dev, yp, tagnames)\n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Reglarization 0.010000: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.77988\n",
      "  Seen 10000 in 48.38 s\n",
      "  [10000]: mean loss 0.313632\n",
      "  Seen 20000 in 98.16 s\n",
      "  [20000]: mean loss 0.272067\n",
      "  Seen 30000 in 147.10 s\n",
      "  [30000]: mean loss 0.26298\n",
      "  Seen 40000 in 195.31 s\n",
      "  [40000]: mean loss 0.246788\n",
      "  [40724]: mean loss 0.234158\n",
      "SGD complete: 40724 examples in 216.47 seconds.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.94      0.99      0.97     42759\n",
      "        LOC       0.88      0.62      0.72      2094\n",
      "       MISC       0.82      0.53      0.64      1268\n",
      "        ORG       0.59      0.39      0.47      2092\n",
      "        PER       0.79      0.70      0.74      3149\n",
      "\n",
      "avg / total       0.91      0.92      0.91     51362\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  76.77%\n",
      "Mean recall:     57.89%\n",
      "Mean F1:         65.71%\n",
      "===== Reglarization 0.001000: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.77988\n",
      "  Seen 10000 in 48.27 s\n",
      "  [10000]: mean loss 0.224718\n",
      "  Seen 20000 in 96.57 s\n",
      "  [20000]: mean loss 0.169044\n",
      "  Seen 30000 in 147.95 s\n",
      "  [30000]: mean loss 0.145005\n",
      "  Seen 40000 in 198.56 s\n",
      "  [40000]: mean loss 0.127767\n",
      "  [40724]: mean loss 0.129319\n",
      "SGD complete: 40724 examples in 219.74 seconds.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.97      0.98      0.98     42759\n",
      "        LOC       0.91      0.77      0.83      2094\n",
      "       MISC       0.83      0.69      0.76      1268\n",
      "        ORG       0.71      0.62      0.66      2092\n",
      "        PER       0.85      0.85      0.85      3149\n",
      "\n",
      "avg / total       0.94      0.95      0.94     51362\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  82.54%\n",
      "Mean recall:     75.18%\n",
      "Mean F1:         78.57%\n",
      "===== Reglarization 0.000500: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.77988\n",
      "  Seen 10000 in 49.86 s\n",
      "  [10000]: mean loss 0.209026\n",
      "  Seen 20000 in 108.58 s\n",
      "  [20000]: mean loss 0.150821\n",
      "  Seen 30000 in 165.50 s\n",
      "  [30000]: mean loss 0.127213\n",
      "  Seen 40000 in 227.60 s\n",
      "  [40000]: mean loss 0.109777\n",
      "  [40724]: mean loss 0.112646\n",
      "SGD complete: 40724 examples in 252.94 seconds.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.97      0.98      0.98     42759\n",
      "        LOC       0.92      0.79      0.85      2094\n",
      "       MISC       0.82      0.71      0.76      1268\n",
      "        ORG       0.71      0.65      0.68      2092\n",
      "        PER       0.86      0.86      0.86      3149\n",
      "\n",
      "avg / total       0.95      0.95      0.95     51362\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  83.07%\n",
      "Mean recall:     77.09%\n",
      "Mean F1:         79.88%\n",
      "===== Reglarization 0.000100: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.77988\n",
      "  Seen 10000 in 50.28 s\n",
      "  [10000]: mean loss 0.19416\n",
      "  Seen 20000 in 101.32 s\n",
      "  [20000]: mean loss 0.127138\n",
      "  Seen 30000 in 158.44 s\n",
      "  [30000]: mean loss 0.104877\n",
      "  Seen 40000 in 303.05 s\n",
      "  [40000]: mean loss 0.0873524\n",
      "  [40724]: mean loss 0.0906175\n",
      "SGD complete: 40724 examples in 342.63 seconds.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.97      0.99      0.98     42759\n",
      "        LOC       0.92      0.81      0.86      2094\n",
      "       MISC       0.79      0.73      0.76      1268\n",
      "        ORG       0.73      0.66      0.69      2092\n",
      "        PER       0.88      0.85      0.86      3149\n",
      "\n",
      "avg / total       0.95      0.95      0.95     51362\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  83.84%\n",
      "Mean recall:     77.68%\n",
      "Mean F1:         80.61%\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Sandbox: build a good model by tuning hyperparameters\n",
    "k = 25\n",
    "reglarizations = [0.01, 0.001, 0.0005, 0.0001]\n",
    "\n",
    "for r in reglarizations:\n",
    "    print \"===== Reglarization %f: =====\" % r \n",
    "    clf = WindowMLP(wv, windowsize=3, dims=[None, 100, 5], reg=r, alpha=0.01)\n",
    "    schedule = NNBase.randomiter(N/k, len(y_train), k)\n",
    "    clf.train_sgd(X=X_train, y=y_train, idxiter=schedule, printevery=10000, costevery=10000)\n",
    "    yp = clf.predict(X_dev)\n",
    "    full_report(y_dev, yp, tagnames)\n",
    "    eval_performance(y_dev, yp, tagnames)\n",
    "\n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Hidden size 50: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.67534\n",
      "  Seen 10000 in 59.63 s\n",
      "  [10000]: mean loss 0.187684\n",
      "  Seen 20000 in 118.72 s\n",
      "  [20000]: mean loss 0.122631\n",
      "  Seen 30000 in 175.24 s\n",
      "  [30000]: mean loss 0.102134\n",
      "  Seen 40000 in 231.35 s\n",
      "  [40000]: mean loss 0.0880059\n",
      "  [40724]: mean loss 0.0863402\n",
      "SGD complete: 40724 examples in 257.27 seconds.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.97      0.98      0.98     42759\n",
      "        LOC       0.89      0.82      0.85      2094\n",
      "       MISC       0.88      0.70      0.78      1268\n",
      "        ORG       0.67      0.67      0.67      2092\n",
      "        PER       0.87      0.84      0.85      3149\n",
      "\n",
      "avg / total       0.95      0.95      0.95     51362\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  82.93%\n",
      "Mean recall:     77.22%\n",
      "Mean F1:         79.86%\n",
      "===== Hidden size 75: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.68243\n",
      "  Seen 10000 in 66.00 s\n",
      "  [10000]: mean loss 0.193231\n",
      "  Seen 20000 in 130.61 s\n",
      "  [20000]: mean loss 0.129677\n",
      "  Seen 30000 in 196.44 s\n",
      "  [30000]: mean loss 0.10171\n",
      "  Seen 40000 in 261.44 s\n",
      "  [40000]: mean loss 0.0863767\n",
      "  [40724]: mean loss 0.0840336\n",
      "SGD complete: 40724 examples in 289.08 seconds.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.96      0.99      0.98     42759\n",
      "        LOC       0.87      0.84      0.85      2094\n",
      "       MISC       0.86      0.72      0.78      1268\n",
      "        ORG       0.79      0.61      0.69      2092\n",
      "        PER       0.92      0.80      0.86      3149\n",
      "\n",
      "avg / total       0.95      0.95      0.95     51362\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  86.71%\n",
      "Mean recall:     75.18%\n",
      "Mean F1:         80.43%\n",
      "===== Hidden size 125: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.64079\n",
      "  Seen 10000 in 83.68 s\n",
      "  [10000]: mean loss 0.199549\n",
      "  Seen 20000 in 164.92 s\n",
      "  [20000]: mean loss 0.132252\n",
      "  Seen 30000 in 247.58 s\n",
      "  [30000]: mean loss 0.10327\n",
      "  Seen 40000 in 329.32 s\n",
      "  [40000]: mean loss 0.0913981\n",
      "  [40724]: mean loss 0.0861366\n",
      "SGD complete: 40724 examples in 359.34 seconds.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.97      0.99      0.98     42759\n",
      "        LOC       0.90      0.83      0.86      2094\n",
      "       MISC       0.87      0.72      0.79      1268\n",
      "        ORG       0.77      0.63      0.69      2092\n",
      "        PER       0.86      0.87      0.87      3149\n",
      "\n",
      "avg / total       0.95      0.95      0.95     51362\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  85.11%\n",
      "Mean recall:     78.00%\n",
      "Mean F1:         81.24%\n",
      "===== Hidden size 150: =====\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.62456\n",
      "  Seen 10000 in 91.07 s\n",
      "  [10000]: mean loss 0.200518\n",
      "  Seen 20000 in 186.03 s\n",
      "  [20000]: mean loss 0.134796\n",
      "  Seen 30000 in 277.71 s\n",
      "  [30000]: mean loss 0.104131\n",
      "  Seen 40000 in 376.28 s\n",
      "  [40000]: mean loss 0.0902652\n",
      "  [40724]: mean loss 0.0869146\n",
      "SGD complete: 40724 examples in 431.79 seconds.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.97      0.99      0.98     42759\n",
      "        LOC       0.90      0.82      0.86      2094\n",
      "       MISC       0.80      0.74      0.77      1268\n",
      "        ORG       0.73      0.64      0.68      2092\n",
      "        PER       0.87      0.83      0.85      3149\n",
      "\n",
      "avg / total       0.95      0.95      0.95     51362\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  83.32%\n",
      "Mean recall:     76.64%\n",
      "Mean F1:         79.82%\n"
     ]
    }
   ],
   "source": [
    "# hidden layer dimension\n",
    "k = 25\n",
    "reglar = 0.0001\n",
    "\n",
    "hiddens = [50, 75, 125, 150]\n",
    "\n",
    "for hidden in hiddens:\n",
    "    print \"===== Hidden size %d: =====\" % hidden \n",
    "    clf = WindowMLP(wv, windowsize=3, dims=[None, hidden, 5], reg=reglar, alpha=0.01)\n",
    "    schedule = NNBase.randomiter(N/k, len(y_train), k)\n",
    "    clf.train_sgd(X=X_train, y=y_train, idxiter=schedule, printevery=10000, costevery=10000)\n",
    "    yp = clf.predict(X_dev)\n",
    "    full_report(y_dev, yp, tagnames)\n",
    "    eval_performance(y_dev, yp, tagnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.64079\n",
      "  Seen 1000 in 15.06 s\n",
      "  [1000]: mean loss 0.33404\n",
      "  Seen 2000 in 33.95 s\n",
      "  [2000]: mean loss 0.314384\n",
      "  Seen 3000 in 53.90 s\n",
      "  [3000]: mean loss 0.288017\n",
      "  Seen 4000 in 73.70 s\n",
      "  [4000]: mean loss 0.267688\n",
      "  Seen 5000 in 88.12 s\n",
      "  [5000]: mean loss 0.252919\n",
      "  Seen 6000 in 107.44 s\n",
      "  [6000]: mean loss 0.251843\n",
      "  Seen 7000 in 144.76 s\n",
      "  [7000]: mean loss 0.224607\n",
      "  Seen 8000 in 164.06 s\n",
      "  [8000]: mean loss 0.215476\n",
      "  Seen 9000 in 179.05 s\n",
      "  [9000]: mean loss 0.206005\n",
      "  Seen 10000 in 193.63 s\n",
      "  [10000]: mean loss 0.199549\n",
      "  Seen 11000 in 207.80 s\n",
      "  [11000]: mean loss 0.187366\n",
      "  Seen 12000 in 231.85 s\n",
      "  [12000]: mean loss 0.180796\n",
      "  Seen 13000 in 249.80 s\n",
      "  [13000]: mean loss 0.170727\n",
      "  Seen 14000 in 266.04 s\n",
      "  [14000]: mean loss 0.155496\n",
      "  Seen 15000 in 282.00 s\n",
      "  [15000]: mean loss 0.161321\n",
      "  Seen 16000 in 298.05 s\n",
      "  [16000]: mean loss 0.150869\n",
      "  Seen 17000 in 315.63 s\n",
      "  [17000]: mean loss 0.14283\n",
      "  Seen 18000 in 337.85 s\n",
      "  [18000]: mean loss 0.138394\n",
      "  Seen 19000 in 361.72 s\n",
      "  [19000]: mean loss 0.133546\n",
      "  Seen 20000 in 383.20 s\n",
      "  [20000]: mean loss 0.132252\n",
      "  Seen 21000 in 403.13 s\n",
      "  [21000]: mean loss 0.129696\n",
      "  Seen 22000 in 423.01 s\n",
      "  [22000]: mean loss 0.12278\n",
      "  Seen 23000 in 443.49 s\n",
      "  [23000]: mean loss 0.116871\n",
      "  Seen 24000 in 463.73 s\n",
      "  [24000]: mean loss 0.12195\n",
      "  Seen 25000 in 483.53 s\n",
      "  [25000]: mean loss 0.118777\n",
      "  Seen 26000 in 503.46 s\n",
      "  [26000]: mean loss 0.108918\n",
      "  Seen 27000 in 523.27 s\n",
      "  [27000]: mean loss 0.108605\n",
      "  Seen 28000 in 542.44 s\n",
      "  [28000]: mean loss 0.107406\n",
      "  Seen 29000 in 561.62 s\n",
      "  [29000]: mean loss 0.107514\n",
      "  Seen 30000 in 581.26 s\n",
      "  [30000]: mean loss 0.10327\n",
      "  Seen 31000 in 600.72 s\n",
      "  [31000]: mean loss 0.101838\n",
      "  Seen 32000 in 620.13 s\n",
      "  [32000]: mean loss 0.0990176\n",
      "  Seen 33000 in 639.72 s\n",
      "  [33000]: mean loss 0.100062\n",
      "  Seen 34000 in 659.29 s\n",
      "  [34000]: mean loss 0.0953783\n",
      "  Seen 35000 in 678.99 s\n",
      "  [35000]: mean loss 0.0963859\n",
      "  Seen 36000 in 698.69 s\n",
      "  [36000]: mean loss 0.0948028\n",
      "  Seen 37000 in 718.47 s\n",
      "  [37000]: mean loss 0.0927377\n",
      "  Seen 38000 in 738.01 s\n",
      "  [38000]: mean loss 0.0921977\n",
      "  Seen 39000 in 757.64 s\n",
      "  [39000]: mean loss 0.0907874\n",
      "  Seen 40000 in 776.85 s\n",
      "  [40000]: mean loss 0.0913981\n",
      "  [40724]: mean loss 0.0861366\n",
      "SGD complete: 40724 examples in 806.25 seconds.\n"
     ]
    }
   ],
   "source": [
    "# best parameter so far\n",
    "k = 25\n",
    "best_reg = 0.0001\n",
    "hidden = 125\n",
    "schedule = NNBase.randomiter(N/k, len(y_train), k)\n",
    "# windowsizes = [4,5,6] window size cannot change, because the X_train[0] shape is predifined as (3, )\n",
    "\n",
    "clf = WindowMLP(wv, windowsize=3, dims=[None, hidden, 5], reg=best_reg, alpha=0.01)\n",
    "traincurvebest = clf.train_sgd(X=X_train, y=y_train, idxiter=schedule, printevery=1000, costevery=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (e): Plot Learning Curves\n",
    "The `train_sgd` function returns a list of points `(counter, cost)` giving the mean loss after that number of SGD iterations.\n",
    "\n",
    "If the model is taking too long you can cut it off by going to *Kernel->Interrupt* in the IPython menu; `train_sgd` will return the training curve so-far, and you can restart without losing your training progress.\n",
    "\n",
    "Make two plots:\n",
    "\n",
    "- Learning curve using `reg = 0.001`, and comparing the effect of changing the learning rate: run with `alpha = 0.01` and `alpha = 0.1`. Use minibatches of size 5, and train for 10,000 minibatches with `costevery=200`. Be sure to scale up your counts (x-axis) to reflect the batch size. What happens if the model tries to learn too fast? Explain why this occurs, based on the relation of SGD to the true objective.\n",
    "\n",
    "- Learning curve for your best model (print the hyperparameters in the title), as trained using your best schedule. Set `costevery` so that you get at least 100 points to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjEAAAGJCAYAAAB7HmJxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/gEB9zrGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107d4fcd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##\n",
    "# Plot your best learning curve here\n",
    "counts, costs = zip(*traincurvebest)\n",
    "figure(figsize=(6,4))\n",
    "plot(5*array(counts), costs, color='b', marker='o', linestyle='-')\n",
    "title(r\"Learning Curve ($\\alpha$=%g, $\\lambda$=%g)\" % (clf.alpha, clf.lreg))\n",
    "xlabel(\"SGD Iterations\"); ylabel(r\"Average $J(\\theta)$\"); \n",
    "ylim(ymin=0, ymax=max(1.1*max(costs),3*min(costs)));\n",
    "ylim(0,0.5)\n",
    "\n",
    "# Don't change this filename!\n",
    "savefig(\"ner.learningcurve.best.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGJCAYAAACpTmgpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSJEmSJEmSJElSXf4f49BCF6ob8vsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107dd7e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##\n",
    "# Plot comparison of learning rates here\n",
    "# feel free to change the code below\n",
    "\n",
    "figure(figsize=(6,4))\n",
    "counts, costs = zip(*trainingcurve1)\n",
    "plot(5*array(counts), costs, color='b', marker='o', linestyle='-', label=r\"$\\alpha=0.01$\")\n",
    "counts, costs = zip(*trainingcurve2)\n",
    "plot(5*array(counts), costs, color='g', marker='o', linestyle='-', label=r\"$\\alpha=0.1$\")\n",
    "title(r\"Learning Curve ($\\lambda=0.01$, minibatch k=5)\")\n",
    "xlabel(\"SGD Iterations\"); ylabel(r\"Average $J(\\theta)$\"); \n",
    "ylim(ymin=0, ymax=max(1.1*max(costs),3*min(costs)));\n",
    "legend()\n",
    "\n",
    "# Don't change this filename\n",
    "savefig(\"ner.learningcurve.comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f): Evaluating your model\n",
    "Evaluate the model on the dev set using your `predict` function, and compute performance metrics below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict labels on the dev set\n",
    "yp = clf.predict(X_dev)\n",
    "# Save predictions to a file, one per line\n",
    "ner.save_predictions(yp, \"dev.predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.97      0.99      0.98     42759\n",
      "        LOC       0.90      0.83      0.86      2094\n",
      "       MISC       0.87      0.72      0.79      1268\n",
      "        ORG       0.77      0.63      0.69      2092\n",
      "        PER       0.86      0.87      0.87      3149\n",
      "\n",
      "avg / total       0.95      0.95      0.95     51362\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  85.11%\n",
      "Mean recall:     78.00%\n",
      "Mean F1:         81.24%\n"
     ]
    }
   ],
   "source": [
    "from nerwindow import full_report, eval_performance\n",
    "full_report(y_dev, yp, tagnames) # full report, helpful diagnostics\n",
    "eval_performance(y_dev, yp, tagnames) # performance: optimize this F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save your predictions on the test set for us to evaluate\n",
    "# IMPORTANT: make sure X_test is exactly as loaded \n",
    "# from du.docs_to_windows, so that your predictions \n",
    "# line up with ours.\n",
    "yptest = clf.predict(X_test)\n",
    "ner.save_predictions(yptest, \"test.predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part [1.1]: Probing neuron responses\n",
    "\n",
    "You might have seen some results from computer vision where the individual neurons learn to detect edges, shapes, or even [cat faces](http://googleblog.blogspot.com/2012/06/using-large-scale-brain-simulations-for.html). We're going to do the same for language.\n",
    "\n",
    "Recall that each \"neuron\" is essentially a logistic regression unit, with weights corresponding to rows of the corresponding matrix. So, if we have a hidden layer of dimension 100, then we can think of our matrix $W \\in \\mathbb{R}^{100 x 150}$ as representing 100 hidden neurons each with weights `W[i,:]` and bias `b1[i]`.\n",
    "\n",
    "### (a): Hidden Layer, Center Word\n",
    "For now, let's just look at the center word, and ignore the rest of the window. This corresponds to columns `W[:,50:100]`, although this could change if you altered the window size for your model. For each neuron, find the top 10 words that it responds to, as measured by the dot product between `W[i,50:100]` and `L[j]`. Use the provided code to print these words and their scores for 5 neurons of your choice. In your writeup, briefly describe what you notice here.\n",
    "\n",
    "The `num_to_word` dictionary, loaded earlier, may be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron 7\n",
      "[ 7078  4227  5071    69  6612 15615  2008  6505  3862  3268]\n",
      "[0]: (2.882) trapped\n",
      "[1]: (2.621) suddenly\n",
      "[2]: (2.561) shelter\n",
      "[3]: (2.339) can\n",
      "[4]: (2.314) deeply\n",
      "[5]: (2.300) cleaner\n",
      "[6]: (2.212) partner\n",
      "[7]: (2.196) accidentally\n",
      "[8]: (2.186) 'd\n",
      "[9]: (2.141) carbon\n",
      "Neuron 17\n",
      "[  59 6714 1743   47 4013 2039 8817 2733  239 7666]\n",
      "[0]: (0.692) there\n",
      "[1]: (0.457) scope\n",
      "[2]: (0.433) planning\n",
      "[3]: (0.407) who\n",
      "[4]: (0.401) difficulty\n",
      "[5]: (0.401) heritage\n",
      "[6]: (0.395) sociology\n",
      "[7]: (0.394) map\n",
      "[8]: (0.380) what\n",
      "[9]: (0.375) geology\n",
      "Neuron 57\n",
      "[  59   45  696   34   47   69   31  589 2509  148]\n",
      "[0]: (1.401) there\n",
      "[1]: (1.223) they\n",
      "[2]: (1.173) whose\n",
      "[3]: (1.150) or\n",
      "[4]: (1.094) who\n",
      "[5]: (1.074) can\n",
      "[6]: (1.070) </s>\n",
      "[7]: (1.050) close\n",
      "[8]: (1.043) hence\n",
      "[9]: (1.033) i\n",
      "Neuron 77\n",
      "[ 3370 24506 23781 35770  5210  8551  2674 39431  4515 20007]\n",
      "[0]: (1.193) carter\n",
      "[1]: (1.156) gaze\n",
      "[2]: (1.111) superficially\n",
      "[3]: (1.079) powerfully\n",
      "[4]: (1.075) pierce\n",
      "[5]: (1.075) alike\n",
      "[6]: (1.014) moore\n",
      "[7]: (1.009) ruddy\n",
      "[8]: (0.998) forth\n",
      "[9]: (0.993) tentatively\n",
      "Neuron 87\n",
      "[ 3861  4381 19334 10906  9409  4529  5357  6543 49658  8551]\n",
      "[0]: (1.557) foster\n",
      "[1]: (1.521) temporarily\n",
      "[2]: (1.475) slough\n",
      "[3]: (1.441) indirectly\n",
      "[4]: (1.422) continually\n",
      "[5]: (1.401) actively\n",
      "[6]: (1.379) grassland\n",
      "[7]: (1.366) solely\n",
      "[8]: (1.358) quicksand\n",
      "[9]: (1.341) alike\n"
     ]
    }
   ],
   "source": [
    "# Recommended function to print scores\n",
    "# scores = list of float\n",
    "# words = list of str\n",
    "def print_scores(scores, words):\n",
    "    for i in range(len(scores)):\n",
    "        print \"[%d]: (%.03f) %s\" % (i, scores[i], words[i])\n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "# W[i,50:100] is（50, ）, L is |V| x 50\n",
    "neurons = [7,17,57,77,87] # change this to your chosen neurons\n",
    "for i in neurons:\n",
    "    print \"Neuron %d\" % i\n",
    "    score = dot(clf.sparams.L, clf.params.W[i,50:100])\n",
    "    index = argsort(score)[-10:][::-1]\n",
    "    print  index\n",
    "    topscores = [score[i] for i in index]\n",
    "    topwords = [num_to_word[i] for i in index]\n",
    "    print_scores(topscores, topwords)\n",
    "    \n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b): Model Output, Center Word\n",
    "Now, let's do the same for the output layer. Here we only have 5 neurons, one for each class. `O` isn't very interesting, but let's look at the other four.\n",
    "\n",
    "Here things get a little more complicated: since we take a softmax, we can't just look at the neurons separately. An input could cause several of these neurons to all have a strong response, so we really need to compute the softmax output and find the strongest inputs for each class.\n",
    "\n",
    "As before, let's consider only the center word (`W[:,50:100]`). For each class `ORG`, `PER`, `LOC`, and `MISC`, find the input words that give the highest probability $P(\\text{class}\\ |\\ \\text{word})$.\n",
    "\n",
    "You'll need to do the full feed-forward computation here - for efficiency, try to express this as a matrix operation on $L$. This is the same feed-forward computation as used to predict probabilities, just with $W$ replaced by `W[:,50:100]`.\n",
    "\n",
    "As with the hidden-layer neurons, print the top 10 words and their corresponding class probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100232, 50) (125, 50)\n",
      "(100232, 125)\n",
      "(5, 125) (5,)\n",
      "(100232, 5)\n",
      "Output neuron 1: LOC\n",
      "[0]: (0.998) italy\n",
      "[1]: (0.998) dune\n",
      "[2]: (0.998) egypt\n",
      "[3]: (0.998) norway\n",
      "[4]: (0.997) malaysia\n",
      "[5]: (0.997) headland\n",
      "[6]: (0.996) zimbabwe\n",
      "[7]: (0.996) lanka\n",
      "[8]: (0.996) switzerland\n",
      "[9]: (0.994) rica\n",
      " \n",
      "Output neuron 2: MISC\n",
      "[0]: (1.000) italian\n",
      "[1]: (1.000) danish\n",
      "[2]: (1.000) israeli\n",
      "[3]: (1.000) brazilian\n",
      "[4]: (1.000) turkish\n",
      "[5]: (1.000) german\n",
      "[6]: (1.000) palestinians\n",
      "[7]: (0.999) egyptian\n",
      "[8]: (0.999) english\n",
      "[9]: (0.999) serb\n",
      " \n",
      "Output neuron 3: ORG\n",
      "[0]: (1.000) corp\n",
      "[1]: (1.000) commons\n",
      "[2]: (1.000) inc\n",
      "[3]: (1.000) microsoft\n",
      "[4]: (1.000) faire\n",
      "[5]: (1.000) maple\n",
      "[6]: (1.000) combine\n",
      "[7]: (1.000) tidewater\n",
      "[8]: (1.000) zenith\n",
      "[9]: (1.000) ajax\n",
      " \n",
      "Output neuron 4: PER\n",
      "[0]: (1.000) innocence\n",
      "[1]: (1.000) martin\n",
      "[2]: (1.000) herb\n",
      "[3]: (1.000) pat\n",
      "[4]: (1.000) stephen\n",
      "[5]: (1.000) jim\n",
      "[6]: (1.000) buck\n",
      "[7]: (1.000) thompson\n",
      "[8]: (1.000) laughter\n",
      "[9]: (1.000) norman\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "def softmax_axis1(x):\n",
    "    x_shape = x.shape\n",
    "\n",
    "    row_max = amax(x, axis=1).reshape((x.shape[0], 1))\n",
    "    x = np.exp(x - row_max)\n",
    "    row_sums = sum(x, axis=1).reshape((x.shape[0], 1))\n",
    "    x = x / row_sums\n",
    "    x = x.reshape(x_shape)\n",
    "    \n",
    "    return x\n",
    "\n",
    "from nn.math import softmax, make_onehot, sigmoid\n",
    "print clf.sparams.L.shape, clf.params.W[:, 50:100].shape\n",
    "z1 = dot(clf.sparams.L, clf.params.W[:, 50:100].T) + clf.params.b1\n",
    "print z1.shape\n",
    "print clf.params.U.shape, clf.params.b2.shape\n",
    "h = 2.0*sigmoid(2.0*z1) - 1.0\n",
    "z2 = dot(h, clf.params.U.T) + clf.params.b2\n",
    "print z2.shape\n",
    "p = softmax_axis1(z2)\n",
    "\n",
    "for i in range(1,5):\n",
    "    # prob = softmax(z2[:,i])\n",
    "    prob = p[:,i]\n",
    "    print \"Output neuron %d: %s\" % (i, num_to_tag[i])\n",
    "    index = argsort(prob)[-10:][::-1] # index of max ten possibility\n",
    "    topprobs = [prob[i] for i in index]\n",
    "    topwords = [num_to_word[i] for i in index]\n",
    "    print_scores(topprobs, topwords)\n",
    "    print \" \"\n",
    "\n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c): Model Output, Preceding Word\n",
    "Now for one final task: let's look at the preceding word. Repeat the above analysis for the output layer, but use the first part of $W$, i.e. `W[:,:50]`.\n",
    "\n",
    "Describe what you see, and include these results in your writeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100232, 125)\n",
      "(5, 125) (5,)\n",
      "(100232, 5)\n",
      "Output neuron 1: LOC\n",
      "[0]: (0.579) mountains\n",
      "[1]: (0.547) visited\n",
      "[2]: (0.482) west\n",
      "[3]: (0.455) east\n",
      "[4]: (0.424) joins\n",
      "[5]: (0.379) enters\n",
      "[6]: (0.375) lake\n",
      "[7]: (0.338) near\n",
      "[8]: (0.325) adjacent\n",
      "[9]: (0.318) inland\n",
      " \n",
      "Output neuron 2: MISC\n",
      "[0]: (0.684) </s>\n",
      "[1]: (0.616) island\n",
      "[2]: (0.494) century\n",
      "[3]: (0.470) series\n",
      "[4]: (0.448) al\n",
      "[5]: (0.434) cave\n",
      "[6]: (0.428) dune\n",
      "[7]: (0.398) atoll\n",
      "[8]: (0.388) county\n",
      "[9]: (0.364) new\n",
      " \n",
      "Output neuron 3: ORG\n",
      "[0]: (0.997) enterprise\n",
      "[1]: (0.996) dream\n",
      "[2]: (0.995) magic\n",
      "[3]: (0.991) grove\n",
      "[4]: (0.989) &\n",
      "[5]: (0.989) venture\n",
      "[6]: (0.987) corporation\n",
      "[7]: (0.985) faire\n",
      "[8]: (0.985) thunder\n",
      "[9]: (0.981) cosmology\n",
      " \n",
      "Output neuron 4: PER\n",
      "[0]: (1.000) pat\n",
      "[1]: (1.000) climber\n",
      "[2]: (1.000) matt\n",
      "[3]: (1.000) egotistical\n",
      "[4]: (1.000) aunt\n",
      "[5]: (1.000) mate\n",
      "[6]: (1.000) stephen\n",
      "[7]: (1.000) peter\n",
      "[8]: (1.000) irascible\n",
      "[9]: (1.000) bob\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "z1 = dot(clf.sparams.L, clf.params.W[:, :50].T) + clf.params.b1\n",
    "print z1.shape\n",
    "print clf.params.U.shape, clf.params.b2.shape\n",
    "h = 2.0*sigmoid(2.0*z1) - 1.0\n",
    "z2 = dot(h, clf.params.U.T) + clf.params.b2\n",
    "print z2.shape\n",
    "p = softmax_axis1(z2)\n",
    "\n",
    "for i in range(1,5):\n",
    "    # prob = softmax(z2[:,i])\n",
    "    prob = p[:,i]\n",
    "    print \"Output neuron %d: %s\" % (i, num_to_tag[i])\n",
    "    index = argsort(prob)[-10:][::-1] # index of max ten possibility\n",
    "    topprobs = [prob[i] for i in index]\n",
    "    topwords = [num_to_word[i] for i in index]\n",
    "    print_scores(topprobs, topwords)\n",
    "    print \" \"\n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
